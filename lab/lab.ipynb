{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b009e9a-e4b2-4adb-b4d8-9cda4988acf3",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "**Goal of the lab**: \n",
    "Given a set of Shakespeare play, can we:\n",
    "- Find similarity across the different plays ?\n",
    "- Find most frequent words per play ?\n",
    "- Characterize the plays and interprete the results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c2044-0659-4931-8558-829d3748f25d",
   "metadata": {},
   "source": [
    "## Pre-requisite\n",
    "To run this lab, you need to have installed on your system:\n",
    "- `pandas`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `spacy` (and the english extension by running `python -m spacy download en_core_web_sm`)\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdc8744-ef07-429e-aeff-ebfc37be695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimsy/Documents/dev/ocr/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbdc95-6945-46ab-bc6e-2f210be97ca3",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "\n",
    "> Reminder from yesterday's session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f297fe-0193-4c10-90b8-e91c6e37ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dec1e-3f96-467d-abae-3ca71ad81bf6",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Load the dataset located in \"data/dataset.csv\"\n",
    "1. Give the number of individuals and list the columns in the dataset.\n",
    "2. Give the number of unique authors in the dataset.\n",
    "3. Give the number of unique school of thoughts in the dataset.\n",
    "4. Give the number of author per school of thoughts in the dataset.\n",
    "5. Plot the number of book by author.\n",
    "6. Plot the number of book by school."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a9f9c-b1db-4da2-96bd-067f9f905940",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "Textual should be:\n",
    "\n",
    "- Lemmed\n",
    "- Cleaned from stop words and punctuation.\n",
    "  \n",
    "We will do it using spacy built-in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6363db4e-2e4d-4ab5-8ce7-2e8f95765519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There there PRON EX expl Xxxxx True True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Take the first sentence from the dataset\n",
    "test_sentence = df[\"sentence_str\"].iloc[0]\n",
    "# Run it through spacy nlp function\n",
    "doc = nlp(test_sentence)\n",
    "\n",
    "# You can now iteratively access the different parsed version of the words\n",
    "for token in doc[:1]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e96fa-6549-48f4-a554-d1f07c30f36c",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Compute for each book the uncapitalized_text (using the `.lower()` function) and lemmed_text from spacy.\n",
    "1. Add 2 new columns to the dataframe: `uncapitalized_text`and `lemmed_text`.\n",
    "3. Add a new column `cleaned_text` which contains the final text: start with the column `lemmed_text` and remove the stop words contained in `spacy.lang.en.stop_words.STOP_WORDS`. You should remove punctuation as well, using the value contained in `is_stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b600-6d0e-440a-a788-00cb1d657fd8",
   "metadata": {},
   "source": [
    "## Analyzing vocabulary use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103292d4-1ff7-4f6a-9ccb-5770d33b8f42",
   "metadata": {},
   "source": [
    "Using the `sklearn` library (see slides in lectures), it is easy to compute word counts using the `CountVectorizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7136707-c336-47b4-8471-99fda23e6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5426f8-3855-4d39-b641-3f6c0aed69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimsy/Documents/dev/ocr/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "count_vect = CountVectorizer(stop_words=list(spacy.lang.en.STOP_WORDS))\n",
    "\n",
    "# Fit on the clean column\n",
    "count_vect.fit(df.sentence_str)\n",
    "\n",
    "# Get count matrix\n",
    "count_matrix = count_vect.transform(df.sentence_str).todense()\n",
    "\n",
    "# Create as dataframe\n",
    "count_df_ = pd.DataFrame(count_matrix, columns=count_vect.get_feature_names_out(), index=df.title)\n",
    "\n",
    "# Merge with df to get information\n",
    "#Â Careful, new column names will have _y appended\n",
    "count_df = count_df_.merge(df, left_index=True, right_on=\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd6de5-9cb6-446c-a67e-afebe164419e",
   "metadata": {},
   "source": [
    "`count_df` is now a dataframe where each column is a word and the rows correspond to the number of occurences of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f647ce-4cc3-44e6-98b8-9f7832753354",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Exercice**:\n",
    "1. Give the 5 most frequent words.\n",
    "2. Find for each book the number of times the word `man` and `woman` is used.\n",
    "3. Find the book that uses most often the word `essence`.\n",
    "4. Find the school of thought that uses most of then word `god`.\n",
    "5. Give for each school of thought the 25 most often used words and print them.\n",
    "6. Perform the same exercise (you can copy paste the code) using tf-idf (`TfidfVectorizer`)\n",
    "7. Create a new list called `columns_to_filter` that contains for each author the 25 words with the highest tf-idf.\n",
    "8. Create a new dataframe called `filtered_df` which contains the previously filtered column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf8d51-a85e-4f5d-91fc-3eeca0732796",
   "metadata": {},
   "source": [
    "## Projecting into a lower space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6f36-cd32-42b3-9a9f-929c5296e255",
   "metadata": {},
   "source": [
    "We are going to use `PCA` (see slides in lecture) to project the different books in a reduced 2 dimensional space and to visually analyze their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce762468-962c-4fee-9d26-cd63ef515f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce into a 2 dimension matrix\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# Train and retrieve output\n",
    "reduced_pca = pca.fit_transform(count_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70564d-62ce-4027-b130-19a348664e3e",
   "metadata": {},
   "source": [
    "**Exercice:**\n",
    "1. Plot the scatter plot into the 2 dimensional space.\n",
    "2. Color the graph per author.\n",
    "3. Add the title of each book using plt.annotate.\n",
    "4. Can you infer anything regarding the distribution in the reduced space ?\n",
    "5. Perform the same exercice on `filtered_df`.\n",
    "\n",
    "**Bonus**: \n",
    "- Find explained variance ratio using the attribute `pca.explained_variance_ratio_`.\n",
    "- Analyze axis meaning by plotting the different components and their associated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa236f01-112b-4260-9a90-79e5bd9a5f08",
   "metadata": {},
   "source": [
    "## Performing k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65c795-7b8c-49dd-8b6d-5797918887f1",
   "metadata": {},
   "source": [
    "We are going to use k-means clustering (see lecture slides) to group works that are the most similar in terms of vocabulary use, using the class `KMeans`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21066cad-6b32-497e-8c06-7519a6698c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimsy/Documents/dev/ocr/venv/lib/python3.9/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans= KMeans(n_clusters=11, n_init=\"auto\")\n",
    "kmeans.fit(count_df_)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc8084-dfcc-44eb-8ffc-b7f0457c5d8f",
   "metadata": {},
   "source": [
    "The `labels` list contains for each title its associated cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd21d9-412a-49d0-9992-ab4a06b75c2c",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Assign the `labels` column to the dataframe.\n",
    "4. Provide the list of titles / author / school within each cluster.\n",
    "5. Give the 10 words with the highest term frequency per cluster.\n",
    "6. Conclude regarding overlap in topics.\n",
    "   \n",
    "**Bonus**:\n",
    "1. Select the optimum number of clusters by analyzing the inertia for each value of k.\n",
    "2. Test using the `DBScan` algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
