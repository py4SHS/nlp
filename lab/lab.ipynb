{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b009e9a-e4b2-4adb-b4d8-9cda4988acf3",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "**Goal of the lab**: \n",
    "Given a set of Shakespeare play, can we:\n",
    "- Find similarity across the different plays ?\n",
    "- Find most frequent words per play ?\n",
    "- Characterize the plays and interprete the results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c2044-0659-4931-8558-829d3748f25d",
   "metadata": {},
   "source": [
    "## Pre-requisite\n",
    "To run this lab, you need to have installed on your system:\n",
    "- `pandas`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `spacy` (and the english extension by running `python -m spacy download en_core_web_sm`)\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdc8744-ef07-429e-aeff-ebfc37be695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbdc95-6945-46ab-bc6e-2f210be97ca3",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "\n",
    "The file `dataset.csv` contains the aggregated data of a set of Shakespeare's play.\n",
    "\n",
    "\n",
    "> This is a reminder from yesterday's session !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f297fe-0193-4c10-90b8-e91c6e37ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dec1e-3f96-467d-abae-3ca71ad81bf6",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Load the dataset located in \"data/dataset.csv\"\n",
    "1. Give the number of individuals and list the columns in the dataset.\n",
    "2. Give the number of unique plays in the dataset and output a list of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a9f9c-b1db-4da2-96bd-067f9f905940",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "Textual should be:\n",
    "\n",
    "- Lemmed\n",
    "- Cleaned from stop words and punctuation.\n",
    "  \n",
    "We will do it using spacy built-in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6363db4e-2e4d-4ab5-8ce7-2e8f95765519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a DET DT det X True True\n",
      "hall hall NOUN NN ROOT xxxx True False\n",
      "in in ADP IN prep xx True True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Take the first sentence from the dataset\n",
    "test_sentence = df[\"PlayerLine\"].iloc[0]\n",
    "# Run it through spacy nlp function\n",
    "doc = nlp(test_sentence)\n",
    "\n",
    "# You can now iteratively access the different parsed version of the words\n",
    "for token in doc[0:3]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e96fa-6549-48f4-a554-d1f07c30f36c",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Compute for each play the uncapitalized_text (using the `.lower()` function) and lemmed_text from spacy.\n",
    "1. Add 2 new columns to the dataframe: `uncapitalized_text`and `lemmed_text`.\n",
    "\n",
    "**Bonus**: Add a new column `cleaned_text` which contains the final text: start with the column `lemmed_text` and remove the stop words contained in `spacy.lang.en.stop_words.STOP_WORDS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b600-6d0e-440a-a788-00cb1d657fd8",
   "metadata": {},
   "source": [
    "## Analyzing vocabulary use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103292d4-1ff7-4f6a-9ccb-5770d33b8f42",
   "metadata": {},
   "source": [
    "Using the `sklearn` library (see slides in lectures), it is easy to compute word counts using the `CountVectorizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7136707-c336-47b4-8471-99fda23e6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5426f8-3855-4d39-b641-3f6c0aed69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "count_vect = CountVectorizer(stop_words=list(spacy.lang.en.STOP_WORDS))\n",
    "\n",
    "# Fit on the clean column\n",
    "count_vect.fit(df.PlayerLine)\n",
    "\n",
    "# Get count matrix\n",
    "count_matrix = count_vect.transform(df.PlayerLine).todense()\n",
    "\n",
    "# Create as dataframe\n",
    "count_df_ = pd.DataFrame(count_matrix, columns=count_vect.get_feature_names_out(), index=df.Play)\n",
    "\n",
    "# Merge with df to get information\n",
    "#Â Careful, new column names will have _y appended\n",
    "count_df = count_df_.merge(df, left_index=True, right_on=\"Play\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd6de5-9cb6-446c-a67e-afebe164419e",
   "metadata": {},
   "source": [
    "`count_df` is now a dataframe where each column is a word and the rows correspond to the number of occurences of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f647ce-4cc3-44e6-98b8-9f7832753354",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Exercice**:\n",
    "1. Give the 5 most frequent words across all plays, using count_df_.\n",
    "2. Find for each play the number of times the word `love` and `death` is used.\n",
    "3. Find the play that uses most often the word `love`.\n",
    "5. Give for each play the most frequent word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf8d51-a85e-4f5d-91fc-3eeca0732796",
   "metadata": {},
   "source": [
    "## Projecting into a lower space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6f36-cd32-42b3-9a9f-929c5296e255",
   "metadata": {},
   "source": [
    "We are going to use `PCA` (see slides in lecture) to project the different books in a reduced 2 dimensional space and to visually analyze their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce762468-962c-4fee-9d26-cd63ef515f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce into a 2 dimension matrix\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Train and retrieve output\n",
    "reduced_pca = pd.DataFrame(pca.fit_transform(count_df_), columns=[\"axis_0\",\"axis_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70564d-62ce-4027-b130-19a348664e3e",
   "metadata": {},
   "source": [
    "**Exercice:**\n",
    "1. Plot the scatter plot into the 2 dimensional space.\n",
    "2. Add the title of each play using plt.annotate.\n",
    "3. Can you infer anything regarding the distribution in the reduced space ?\n",
    "\n",
    "**Bonus**: \n",
    "- Find explained variance ratio using the attribute `pca.explained_variance_ratio_` and conclude regarding relevance of axis choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa236f01-112b-4260-9a90-79e5bd9a5f08",
   "metadata": {},
   "source": [
    "## Performing k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65c795-7b8c-49dd-8b6d-5797918887f1",
   "metadata": {},
   "source": [
    "We are going to use k-means clustering (see lecture slides) to group works that are the most similar in terms of vocabulary use, using the class `KMeans`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21066cad-6b32-497e-8c06-7519a6698c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans= KMeans(n_clusters=5, n_init=\"auto\")\n",
    "kmeans.fit(count_df_)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc8084-dfcc-44eb-8ffc-b7f0457c5d8f",
   "metadata": {},
   "source": [
    "The `labels` list contains for each title its associated cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd21d9-412a-49d0-9992-ab4a06b75c2c",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Assign the `labels` column to the dataframe `count_df`.\n",
    "2. Provide the list of plays within each cluster.\n",
    "3. Give the 10 words with the highest term frequency per cluster.\n",
    "4. Conclude regarding vocabulary overlap in Shakespeare's play.\n",
    "   \n",
    "**Bonus**:\n",
    "1. Select the optimum number of clusters by analyzing the inertia for each value of k.\n",
    "2. Test using the `DBScan` algorithm and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7317b20",
   "metadata": {},
   "source": [
    "**Bonus**: document embedding using Doc2Vec.\n",
    "\n",
    "Another possible approach to embedding is the use of *neural networks* (we'll see tomorrow the practical behind these terms).\n",
    "Many models exist for this embedding, and one of the most popular one is Doc2Vec.\n",
    "This model is available in the library `gensim` (you can access the documentation here: https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "> Do not forget to install `gensim` if you want this code to run !\n",
    "> There is a slight hiccup as `gensim` requires a lower version of scipy, so you need additionally to run: `pip install scipy==1.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063531ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df.PlayerLine.values.tolist())]\n",
    "\n",
    "# vector_size controls the dimension of the embeddings\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "embeddings = pd.DataFrame([model.infer_vector(doc.split()) for doc in df.PlayerLine.values.tolist()], index=count_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ef45b",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Perform the clustering using k-means on the neural networks embedding and compare it to using count/tf-idf embedder.\n",
    "2. Set the `vector_size` argument to 2 in order to get 2 dimension embeddings.\n",
    "3. Plot the 2D embeddings, color and annotate according to play name and draw conclusions regarding plays similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
